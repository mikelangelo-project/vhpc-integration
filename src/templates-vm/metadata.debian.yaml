#cloud-config
#
# Copyright 2015 HLRS, University of Stuttgart
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#
# set the hostname the same as the physical node, but prefix it with 'v'
#
hostname: __VHOSTNAME__


#
# security
#
disable_root: true
ssh_pwauth: false


#
# update VM
#  security wise it is very desired, however it cannot be recommended
#  since it may break user compiled binaries inside the VM
#  (and delays the boot time)
#
package_upgrade: false


#
# add the user and its group with exactly same UID/GID (crucial for NFS access)
#
bootcmd:
 - groupadd -g __GROUP_ID__ __USER_NAME__
 - useradd -u __USER_ID__ -g __GROUP_ID__ -s /bin/bash -M __USER_NAME__
# Create the workspace mount point in case it is not present
 - mkdir /workspace
 - modprobe 9p
 - modprobe 9pnet
 - modprobe 9pnet_virtio


#
# install missing packages
# for NFS, OpenMPI, libMetis and 9p support
#
packages:
 - openssh-server
 - nfs-common
 - libnfs1
 - openmpi-bin
 - libopenmpi-dev
 - libmetis5
 - libmetis-dev
 - 9mount


#
# mount the NFS shares
#
mounts:
 - [ "172.18.2.5:/home", /home, "nfs", "rw,intr,noatime", "0", "0" ]
 - [ "172.18.2.3:/storage/mikelangelo/ssd_data/opt", /opt, "nfs", "rw,intr,noatime", "0", "0" ]
 - [ "172.18.2.3:/storage/mikelangelo/ssd_scratch", /workspace, "nfs", "rw,intr,noatime", "0", "0" ]
 # following line makes PBS_NODEFILE available inside VM (mount-point is defined in the domain.xml)
 - [ /var/spool/torque/aux, "/var/spool/torque/aux", "9p", "trans=virtio,version=9p2000.L,ro,chmod=0444,uid=0,guid=0", "0", "0" ]
 # following line make the pbs env available via profile.d (mount-point is defined in the domain.xml)
 - [ /var/spool/torque/vm, "/var/spool/torque/vm", "9p", "trans=virtio,version=9p2000.L,ro,chmod=0444,uid=0,guid=0", "0", "0" ]


#
# DNS
#

# DNS
manage-resolv-conf: true
resolv_conf:
  nameservers:
    - '172.18.2.2'
  searchdomains:
    - rus.uni-stuttgart.de
  domain: rus.uni-stuttgart.de
#  options:
#    option1: value1
#    option2: value2
#    option3: value3


#
# create files
#
write_files:
 # NTP config
 - path: "/etc/ntp.conf"
   permissions: "0644"
   owner: "root"
   encoding: "text/plain"
   content: |
     # Common pool
     server rustime01.rus.uni-stuttgart.de
     server rustime02.rus.uni-stuttgart.de
     # - Allow only time queries, at a limited rate.
     # - Allow all local queries (IPv4, IPv6)
     restrict default nomodify nopeer noquery limited kod
     restrict 127.0.0.1
     restrict [::1]
 # SSH server config
 - path: /etc/ssh/sshd_config
   content: |
         Port 22
         Protocol 2
         HostKey /etc/ssh/ssh_host_rsa_key
         HostKey /etc/ssh/ssh_host_dsa_key
         HostKey /etc/ssh/ssh_host_ecdsa_key
         HostKey /etc/ssh/ssh_host_ed25519_key
         UsePrivilegeSeparation yes
         KeyRegenerationInterval 3600
         ServerKeyBits 1024
         SyslogFacility AUTH
         LogLevel INFO
         LoginGraceTime 120
         PermitRootLogin yes
         StrictModes yes
         RSAAuthentication yes
         PubkeyAuthentication yes
         PasswordAuthentication no
         IgnoreRhosts yes
         RhostsRSAAuthentication no
         HostbasedAuthentication no
         PermitEmptyPasswords no
         ChallengeResponseAuthentication no
         X11Forwarding yes
         X11DisplayOffset 10
         PrintMotd no
         PrintLastLog yes
         TCPKeepAlive yes
         AcceptEnv LANG LC_*
         Subsystem sftp /usr/lib/openssh/sftp-server
         UsePAM yes
         AllowUsers __USER_NAME__
         AllowUsers nico
 # PBS job env file
 - path: "/etc/profile.d/pbsVirtualJobEnv.sh"
   permissions: "0644"
   owner: "root"
   encoding: "text/plain"
   content: |
     #!bin/bash
     file="/var/spool/torque/vm/vmJobEnvironment";
     [ -f "$file" ] && source $file;
 # create starter script for root VM {pro,epi}logue[.parallel]
 # this way we do not need a root access afterwards to the VM instance
 - path: "/usr/local/sbin/pbs-vm-scripts"
   permissions: "0774"
   owner: "root"
   encoding: "text/plain"
   content: |
     #!bin/bash
     RUID=__RUID__;
     JOBID=$PBS_JOBID;
     SCRIPT_BASE_DIR=__SCRIPT_BASE_DIR__;
     source /etc/profile.d/pbsVirtualJobEnv.sh;
     source $SCRIPT_BASE_DIR/common/config.sh;
     source $SCRIPT_BASE_DIR/common/functions.sh;
     if [ -f "$PBS_NODEFILE" ]; then
       rank0VM="$(head -n1 $PBS_NODEFILE)";
     else
       rank0VM="";
     fi
     if [ ! -n "$rank0VM" ]; then
       logErrorMsg "Failed to get rank0";
     fi
     if [ $# -ne 1 ];then
       logErrorMsg "usage: $0 [epilogue|prologue]";
     fi
     if [ "$1" == "prologue" ] \
          && [ systemctl list-units --type target | grep runlevel4.target ] \
          && [ systemctl list-units --type target | grep multi-user.target ]; then #assume we are booting
       scriptPrefix="pro";
     elif [ "$1" == "epilogue" ] \
          && [ systemctl list-units --type target | grep multi-user.target ] \
          && [ systemctl list-units --type target | grep runlevel0.target ]; then #assume we are stopping
       scriptPrefix="epi";
     else
       logErrorMsg "Wrong system state for requested script execution.";
     fi
     if [ -n "$(ip a | grep $rank0VM)" ] \
          || [ -n "$(hostname | grep $rank0VM)" ]; then
       $SCRIPT_BASE_DIR/vm_scripts/${scriptPrefix}logue;
     else
       $SCRIPT_BASE_DIR/vm_scripts/${scriptPrefix}logue.parallel;
     fi
     dhclient -r;
 #
 # create systemd service script for VM {pro,epi}logue.parallel runner
 #
 - path: "/lib/systemd/system/pbs-vm-prologue.service"
   permissions: "0664"
   owner: "root"
   encoding: "text/plain"
   content: |
     [Unit]
     Description=PBS VM script runner service for prologue
     After=syslog.target network.target auditd.service sshd.service cloud-init.service
     Conflicts=/usr/local/sbin/pbs-vm-epilogue.precancel
     
     [Service]
     ExecStart=/usr/local/sbin/pbs-vm-scripts prologue
     ExecStop=kill $(cat /var/spool/pbs_vm_scriptd/pid) && /usr/local/sbin/pbs-vm-epilogue.precancel
     Type=forking
     KillMode=process
     PIDFile=/var/spool/pbs_vm_scriptd/pid
     
     [Install]
     WantedBy=multi-user.target
 #
 # create systemd service script for VM {pro,epi}logue.parallel runner
 #
 - path: "/lib/systemd/system/pbs-vm-epilogue.service"
   permissions: "0664"
   owner: "root"
   encoding: "text/plain"
   content: |
     [Unit]
     Description=PBS VM script runner service for epilogue
     After=syslog.target network.target auditd.service sshd.service cloud-init.service
     Conflicts=/usr/local/sbin/pbs-vm-epilogue.precancel
     
     [Service]
     ExecStart=/usr/local/sbin/pbs-vm-scripts epilogue
     Type=forking
     KillMode=process
     PIDFile=/var/spool/pbs_vm_scriptd/pid
     
     [Install]
     WantedBy=multi-user.target
 #
 # create wrapper script for mpirun
 #
 - path: "/usr/local/bin/mpirun"
   permissions: "0555"
   owner: "root"
   encoding: "text/plain"
   content: |
     #!/bin/bash
     set +o nounset;
     params=$@;
     nodeFile=$PBS_NODEFILE;
     # ensure we call the binary in case the runcmd doesn't work as desired
     if [ ! -n "$(which mpirun)" ]; then
       echo "VM mpirun wrapper ERROR: No mpirun found.";
       exit 1;
     fi
     #unset all PBS env vars in script's scope (parent scope is not affected)
     for enVar in $(env | grep PBS_); do
       unset "$( echo $enVar | cut -d'=' -f1)";
     done
     if [[ $params =~ -H|-host|--host|-hostfile|--hostfile-default-hostfile|--default-hostfile ]]; then
       # no need to do anything else
       /usr/bin/mpirun $params;
     else # append hostsfile to mpirun cmd
       /usr/bin/mpirun --hostfile $nodeFile $params;
     fi
     exit $?;
 #
 # profile for mpirun
 #
 - path: /etc/profile.d/00-mpirun_wrapper.sh
   permissions: 0644
   content: |
     # ensure the wrapper is found before the mpirun executable
     export PATH=/usr/local/bin/mpirun:$PATH;
 #
 # profile for HPC stack
 #
 - path: /etc/profile.d/99-mikelangelo-hpc_stack.sh
   permissions: 0644
   content: |
     export SCRIPT_BASE_DIR=__SCRIPT_BASE_DIR__;

#
# ping the physical host, otherwise it cannot see the VM's IP with the help of
# 'arp -an's and create a profile for the virtual job environment
#
runcmd:
 - [ dpkg-reconfigure, openssh-server ]
 - [ ping, -c1, __HOSTNAME__ ]
 - [ /usr/local/bin/pbs-vm-scripts ]
 - [ systemctl, enable, ssh.service ]
 - [ systemctl, start, ssh.service ]
 - [ systemctl, start, pbs-vm-prologue.service ]
 - [ systemctl, enable, pbs-vm-epilogue.service ]

#
# write everything into a dedicated log file
#
output: {all: '| tee -a /var/log/cloud-init-output.log'}

# final_message
final_message: "The system is finally up, after $UPTIME seconds"

